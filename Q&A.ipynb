{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pDOcXX5NlAf"
      },
      "source": [
        "# ** ارائه یک چارچوب برای پاسخ به پرسش‌های قرآنی با ایجاد روابط پنج تایی **\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxDKOne0uGBd"
      },
      "outputs": [],
      "source": [
        "!pip install dadmatools\n",
        "!pip install hazm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NyOyfOLp_Zf"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\n",
        "!mkdir resources\n",
        "!unzip /content/resources-0.5.zip -d /content/resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7uIzpjcN2K9"
      },
      "source": [
        "*normalizer, tokenizer, lemmatizer, pos tagger, dependancy parser.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7P72ttdCGC6"
      },
      "outputs": [],
      "source": [
        "from dadmatools.models.normalizer import Normalizer\n",
        "\n",
        "normalizer = Normalizer(\n",
        "    full_cleaning=False,\n",
        "    unify_chars=True,\n",
        "    refine_punc_spacing=True,\n",
        "    remove_extra_space=True,\n",
        "    remove_puncs=False,\n",
        "    remove_html=False,\n",
        "    remove_stop_word=False,\n",
        "    replace_email_with=\"<EMAIL>\",\n",
        "    replace_number_with=None,\n",
        "    replace_url_with=\"\",\n",
        "    replace_mobile_number_with=None,\n",
        "    replace_emoji_with=None,\n",
        "    replace_home_number_with=None\n",
        ")\n",
        "\n",
        "text = \"\"\"\n",
        "اللّه، كه جز او معبودى نیست، زنده و برپادارنده است. نه خوابى سبك او را فرا گیرد و نه خوابى سنگین. آنچه در آسمان ها و آنچه در زمین است، از آنِ اوست. كیست آنكه جز به اذن او در پیشگاهش شفاعت كند. گذشته و آینده آنان را مى داند. و به چیزى از علم او احاطه پیدا نمى كنند مگر به مقدارى كه او بخواهد. كرسى (علم و قدرت) او آسمان ها و زمین را در برگرفته و نگهدارى آن دو بر او سنگین نیست و او والا و بزرگ است.\n",
        "\"\"\"\n",
        "#print('input text : ', text)\n",
        "#print('out2 text when replace emails and remove urls : ', normalizer.normalize(text))\n",
        "\n",
        "#full cleaning\n",
        "#normalizer = Normalizer(full_cleaning=True)\n",
        "##print('out2 text when using full_cleaning parameter', normalizer.normalize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97jk-_-ogmKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875afa52-d93e-4da3-9002-df063e3a0bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flEghMJ-pe_r"
      },
      "outputs": [],
      "source": [
        "from dadmatools.datasets import get_all_datasets_info, get_dataset_info\n",
        "from dadmatools.datasets import ARMAN\n",
        "from dadmatools.datasets import TEP\n",
        "from dadmatools.datasets import PerSentLexicon\n",
        "from dadmatools.datasets import FaSpell\n",
        "from dadmatools.datasets import WikipediaCorpus\n",
        "from dadmatools.datasets import PersianNer\n",
        "from dadmatools.datasets import PersianNews\n",
        "from dadmatools.datasets import PnSummary\n",
        "from dadmatools.datasets import FarsTail\n",
        "from dadmatools.datasets import SnappfoodSentiment\n",
        "from dadmatools.datasets import get_all_datasets_info\n",
        "from dadmatools.datasets import Peyma\n",
        "from dadmatools.datasets import PerUDT\n",
        "from dadmatools.datasets import PersianTweets\n",
        "from pprint import pprint\n",
        "import re, string \n",
        "import pandas as pd   \n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format('drive/MyDrive/Thesis/model.bin', binary=True)\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from hazm import *\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "EsaHF9PhgSz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIzu-hXvnYoF"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText\n",
        "!pip install fastText\n",
        "from dadmatools.embeddings import get_embedding, get_all_embeddings_info, get_embedding_info\n",
        "pprint(get_all_embeddings_info())\n",
        "pprint(get_embedding_info('word2vec-conll'))\n",
        "embedding = get_embedding('word2vec-conll')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlnZ9dKGR_X0"
      },
      "outputs": [],
      "source": [
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "# here lemmatizer and pos tagger will be loaded\n",
        "# as tokenizer is the default tool, it will be loaded even without calling\n",
        "# pips = 'lem,pos' \n",
        "# pips = 'dep' \n",
        "pips = 'tok,lem,pos,dep,cons' \n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "# you can see the pipeline with this code\n",
        "#print(nlp.analyze_pipes(pretty=True))\n",
        "\n",
        "# doc is an SpaCy object\n",
        "#doc = nlp(' این سوره یكصد و بیست آیه دارد و چند ماه پیش از رحلت پیامبر اكرم صلى الله علیه وآله نازل شده و هیچ یك از آیات آن نسخ نگشته است. این سوره به خاطر دعاى حضرت عیسى علیه السلام براى نزول مائده ى آسمانى كه در آیه ى 114 آمده، «مائده» نامیده شده است. البتّه به «عهد» و «عقود» نیز نام گذارى شده و در مجموع آیاتِ این سوره نسبت به یك پیمان شكنى مهم هشدار مى دهد.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2lPrHNkuKm6"
      },
      "outputs": [],
      "source": [
        "def segmentation_verb(sen1):\n",
        "  verb__=[]\n",
        "\n",
        "  #print(\"segmentation_verb\",sen1)\n",
        "  Sentence=''\n",
        "  l1=len(sen1[0])\n",
        "  l2=l1\n",
        "  #print(len(sen1[0]))\n",
        "  for i in range(len(sen1[0])):\n",
        "    if i< l1 and (sen1[0][i]['pos']=='VERB' and sen1[0][i]['pos']!='AUX') or ((sen1[0][i]['rel']=='cop' or sen1[0][i]['rel']=='ccomp') and sen1[0][i]['pos']=='AUX') or (sen1[0][i]['rel']=='aux' and sen1[0][i]['pos']=='AUX') or (sen1[0][i]['rel']=='root' and sen1[0][i]['pos']=='AUX') or (sen1[0][i]['text']=='،' and sen1[0][i+1]['rel']=='nsubj') :\n",
        "      verb__.append(sen1[0][i]['id'])\n",
        "\n",
        " \n",
        "  start=0\n",
        "  sentn=[]\n",
        "  for i in verb__:\n",
        "    #print(i)\n",
        "    for j in range(start, i):\n",
        "      Sentence= Sentence + ' ' + sen1[0][j]['text']\n",
        "    start=i\n",
        "    sentn.append(Sentence)\n",
        "    Sentence=''\n",
        "  if start < (len(sen1[0])-2):\n",
        "    for j in range(start, len(sen1[0])):\n",
        "      Sentence= Sentence + ' ' + sen1[0][j]['text']\n",
        "    sentn.append(Sentence)\n",
        "    Sentence=''\n",
        "    #print(\"sentnsentnsentnsentnsentnsentnsentnsentnsentn\",sentn)\n",
        "\n",
        "  if len(sentn)==1:\n",
        "    sentn=[]\n",
        "\n",
        "  return sentn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYrheIJxkdYl"
      },
      "outputs": [],
      "source": [
        "def to_json1(pipelines, doc):\n",
        "    dict_list = []\n",
        "    for sent in doc._.sentences:\n",
        "        sentence = []\n",
        "        for i, d in enumerate(sent):\n",
        "            dictionary = {}\n",
        "            dictionary['id'] = i+1\n",
        "            dictionary['text'] = d.text\n",
        "\n",
        "            if 'dep' in pipelines: \n",
        "                dictionary['rel'] = d.dep_\n",
        "                dictionary['root'] = d._.dep_arc\n",
        "            sentence.append(dictionary)\n",
        "        dict_list.append(sentence)\n",
        "    return dict_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDKPO9-pTmZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5746fd0b-ffd0-4eaf-abc3-76e0031701d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fa_tokenizer exists in /root/.pernlp/fa_tokenizer.pt\n",
            "Model fa_mwt exists in /root/.pernlp/fa_mwt.pt\n",
            "Model fa_lemmatizer exists in /root/.pernlp/fa_lemmatizer.pt\n",
            "Model parsbert exists in /root/.pernlp/parsbert.tar.gz\n",
            "Model dependencyparser exists in /root/.pernlp/dependencyparser.pt\n",
            "2023-02-19 22:46:46,744 loading file /usr/local/lib/python3.8/dist-packages/dadmatools/saved_models/dependencyparser/dependencyparser.pt\n",
            "Model parsbert exists in /root/.pernlp/parsbert.tar.gz\n",
            "Model postagger exists in /root/.pernlp/postagger.pt\n",
            "2023-02-19 22:46:58,379 loading file /usr/local/lib/python3.8/dist-packages/dadmatools/saved_models/postagger/postagger.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-02-19 22:47:06,379 INFO] [Ensembling dict with seq2seq lemmatizer...]\n"
          ]
        }
      ],
      "source": [
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "# here lemmatizer and pos tagger will be loaded\n",
        "# as tokenizer is the default tool, it will be loaded even without calling\n",
        "# pips = 'lem,pos' \n",
        "# pips = 'dep' \n",
        "pips = 'tok,lem,pos,dep' \n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "# you can see the pipeline with this code\n",
        "#print(nlp.analyze_pipes(pretty=True))\n",
        "text_=\"\"\"چه کسی نوزاد خود را صبح با حالت نگران به رود انداخت\"\"\"\n",
        "# doc is an SpaCy object\n",
        "doc = nlp(text_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRKddLOBTwqs"
      },
      "outputs": [],
      "source": [
        "output = language.to_json(pips, doc)\n",
        "#print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNpsfUhLP9hm"
      },
      "outputs": [],
      "source": [
        "def parser(text_):\n",
        " \n",
        "  doc = nlp(text_)\n",
        "  out2 = language.to_json(pips, doc)\n",
        "  return out2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmUhMqJWAQeh"
      },
      "outputs": [],
      "source": [
        "def rootid_(out2):\n",
        "  rootid=[]\n",
        "  test_list = out2\n",
        "  remov=[]\n",
        "  l1=len(out2[0])\n",
        "  l2=l1\n",
        "  rootfirst=''\n",
        "  for i in range(len(out2[0])):\n",
        "    if i< l1 and (out2[0][i]['rel']=='root' or (out2[0][i]['pos']=='VERB' and out2[0][i]['root']==out2[0][i]['id'])):\n",
        "      rootid.append(out2[0][i]['id'])\n",
        "      if rootfirst=='':\n",
        "        rootfirst=out2[0][i]['id']\n",
        "  # اگر فعل و ریشه جمله تشخیص داده نشد به روش زیر عمل می کنیم\n",
        "  if not rootid :\n",
        "    rootlist=[]\n",
        "    for i in range(len(out2[0])):\n",
        "      rootlist.append(out2[0][i]['root'])\n",
        "    idlist=most_frequent(rootlist)\n",
        "    #print(\"list\",rootlist,\"most_frequent\",idlist)\n",
        "    rootid.append(out2[0][idlist-1]['id'])\n",
        "    if rootfirst=='':\n",
        "      rootfirst=out2[0][idlist-1]['id']\n",
        "    out2[0][idlist-1]['root']=0\n",
        "    #print(out2)\n",
        "      \n",
        "\n",
        "    \n",
        "  #print(\"rootfirst\",rootfirst)\n",
        "  return rootid,rootfirst,out2\n",
        "\n",
        "def most_frequent(List):\n",
        "    return max(set(List), key = List.count)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNWUO5symsPo"
      },
      "outputs": [],
      "source": [
        "def nsubj_obl_obj(out2,rootfirst):\n",
        "  rootnsubj=[]\n",
        "  rootobl=[]\n",
        "  rootobj=[]\n",
        "  l1=len(out2[0])\n",
        "  l2=l1\n",
        "  List_obl = ['ADV','advmod','obl','obl:arg']\n",
        "  \n",
        "  #print(\"5%%%%3333333333%%%\",rootnsubj)\n",
        "  for i in range(len(out2[0])):\n",
        "    #این شرط وجود اولین فاعل است if not rootnsubj\n",
        "    if not rootnsubj and i< l1 and (out2[0][i]['rel']=='nsubj' or out2[0][i]['rel']=='nsubj:pass') :\n",
        "      rootnsubj.append(out2[0][i]['id'])\n",
        "   \n",
        "    elif i< l1 and  (out2[0][i]['rel'] in List_obl) :\n",
        "      rootobl.append(out2[0][i]['id'])\n",
        "     \n",
        "    elif i< l1  and out2[0][i]['rel']=='obj' :\n",
        "      rootobj.append(out2[0][i]['id'])\n",
        "\n",
        "  if not rootnsubj and (out2[0][0]['pos']=='NOUN' or out2[0][0]['pos']=='PROPN') and out2[0][0]['rel']=='nmod' and out2[0][0]['root']==rootfirst :\n",
        "    rootnsubj.append(out2[0][0]['id'])\n",
        "    out2[0][0]['rel']='nsubj'\n",
        "    #print(\"5%%%%%%%\",rootnsubj)\n",
        "  #print(\"5%%%%%%%1111111111\",rootnsubj)\n",
        "  rootobl1=[]\n",
        "  rootobl2=[]\n",
        "  rootobl3=[]\n",
        "  rootobl4=[]\n",
        "  if len(rootobl)>0:\n",
        "    rootobl1.append(rootobl[0])\n",
        "  if len(rootobl)>1:\n",
        "    rootobl2.append(rootobl[1])\n",
        "  if len(rootobl)>2:\n",
        "    rootobl3.append(rootobl[2])\n",
        "  if len(rootobl)>3:\n",
        "    rootobl4.append(rootobl[3])  \n",
        "  return out2,rootnsubj,rootobj,rootobl1,rootobl2,rootobl3,rootobl4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZA_sZiv9Zr8"
      },
      "outputs": [],
      "source": [
        "def duplicates(list1):\n",
        "  seen = set()\n",
        "  uniq = []\n",
        "  for x in list1:\n",
        "      if x not in seen:\n",
        "          uniq.append(x)\n",
        "          seen.add(x)\n",
        "  return uniq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKW8EIoKx2Fq"
      },
      "outputs": [],
      "source": [
        "def nsubj_obl_obj_full(out2,rootnsubj,rootobl1,rootobl2,rootobl3,rootobl4,rootobj,rootid):\n",
        "  for r in range(len(out2[0])):\n",
        "    l1=len(out2[0])\n",
        "    l2=l1\n",
        "    remov=[]\n",
        "    for i in range(len(out2[0])):\n",
        "      for j in range(len(rootnsubj)):\n",
        "        if i< l1 and out2[0][i]['root']==rootnsubj[j] and out2[0][i]['id'] != rootnsubj[j]:\n",
        "          if out2[0][i]['id'] > rootnsubj[j] :\n",
        "            rootnsubj.append(out2[0][i]['id'])\n",
        "            rootnsubj=duplicates(rootnsubj)\n",
        "          \n",
        "          elif out2[0][i]['id'] < rootnsubj[j] :\n",
        "            rootnsubj.append(out2[0][i]['id'])\n",
        "            rootnsubj=duplicates(rootnsubj)\n",
        "          \n",
        "\n",
        "      for j in range(len( rootobl1)):\n",
        "        if i< l1 and out2[0][i]['root']== rootobl1[j] and out2[0][i]['id'] != rootobl1[j]:\n",
        "          if out2[0][i]['id'] >  rootobl1[j] :\n",
        "             rootobl1.append(out2[0][i]['id'])\n",
        "             rootobl1=duplicates( rootobl1)\n",
        "          \n",
        "          elif out2[0][i]['id'] <  rootobl1[j] :\n",
        "             rootobl1.append(out2[0][i]['id'])\n",
        "             rootobl1=duplicates( rootobl1)\n",
        "        \n",
        "      for j in range(len( rootobl2)):\n",
        "        if i< l1 and out2[0][i]['root']== rootobl2[j] and out2[0][i]['id'] != rootobl2[j]:\n",
        "          if out2[0][i]['id'] >  rootobl2[j] :\n",
        "             rootobl2.append(out2[0][i]['id'])\n",
        "             rootobl2=duplicates( rootobl2)\n",
        "          \n",
        "          elif out2[0][i]['id'] <  rootobl2[j] :\n",
        "             rootobl2.append(out2[0][i]['id'])\n",
        "             rootobl2=duplicates( rootobl2)\n",
        "\n",
        "      for j in range(len( rootobl3)):\n",
        "        if i< l1 and out2[0][i]['root']== rootobl3[j] and out2[0][i]['id'] != rootobl3[j]:\n",
        "          if out2[0][i]['id'] >  rootobl3[j] :\n",
        "             rootobl3.append(out2[0][i]['id'])\n",
        "             rootobl3=duplicates( rootobl3)\n",
        "          \n",
        "          elif out2[0][i]['id'] <  rootobl3[j] :\n",
        "             rootobl3.append(out2[0][i]['id'])\n",
        "             rootobl3=duplicates( rootobl3)\n",
        "\n",
        "      for j in range(len( rootobl4)):\n",
        "        if i< l1 and out2[0][i]['root']== rootobl4[j] and out2[0][i]['id'] != rootobl4[j]:\n",
        "          if out2[0][i]['id'] >  rootobl4[j] :\n",
        "             rootobl4.append(out2[0][i]['id'])\n",
        "             rootobl4=duplicates( rootobl4)\n",
        "          \n",
        "          elif out2[0][i]['id'] <  rootobl4[j] :\n",
        "             rootobl4.append(out2[0][i]['id'])\n",
        "             rootobl4=duplicates( rootobl4)         \n",
        "\n",
        "      for j in range(len(rootobj)):\n",
        "        if i< l1 and out2[0][i]['root']==rootobj[j] and out2[0][i]['id'] != rootobj[j]:\n",
        "          \n",
        "          if out2[0][i]['id'] > rootobj[j] :\n",
        "            rootobj.append(out2[0][i]['id'])\n",
        "            rootobj=duplicates(rootobj)\n",
        "        \n",
        "          elif out2[0][i]['id'] < rootobj[j] :\n",
        "            rootobj.append(out2[0][i]['id'])\n",
        "            rootobj=duplicates(rootobj)\n",
        "        \n",
        "\n",
        "\n",
        "      for j in range(len(rootid)):\n",
        "        if i< l1 and out2[0][i]['root']==rootid[j] and out2[0][i]['id'] != rootid[j] and (out2[0][i]['rel'] not in ('nsubj','obj','obl:arg','obl')):\n",
        "          if out2[0][i]['id'] > rootid[j] :\n",
        "            rootid.append(out2[0][i]['id'])\n",
        "            rootid=duplicates(rootid)\n",
        "          \n",
        "          elif out2[0][i]['id'] < rootid[j] :\n",
        "            rootid.append(out2[0][i]['id'])\n",
        "            rootid=duplicates(rootid)\n",
        "  return rootnsubj,rootobj,rootobl1,rootobl2,rootobl3,rootobl4,rootid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6dPSclmDsBc"
      },
      "outputs": [],
      "source": [
        "def pars_text(out2,rootnsubj,rootobl1,rootobl2,rootobl3,rootobl4,rootobj,rootid): \n",
        "  nsubj=''\n",
        "  obl1=''\n",
        "  obl2=''\n",
        "  obl3=''\n",
        "  obl4=''\n",
        "  obj=''\n",
        "  verb=''\n",
        "  \n",
        "  for i in range(len(out2[0])):\n",
        "    for j in rootid:\n",
        "      if  out2[0][i]['id']==j :\n",
        "        verb= verb + ' ' + out2[0][i]['text']\n",
        "    for j in  rootobl1:\n",
        "      if  out2[0][i]['id']==j :\n",
        "        obl1= obl1 + ' ' + out2[0][i]['text']\n",
        "    for j in  rootobl2:\n",
        "      if  out2[0][i]['id']==j :\n",
        "        obl2= obl2 + ' ' + out2[0][i]['text']\n",
        "    for j in  rootobl3:\n",
        "      if  out2[0][i]['id']==j :\n",
        "        obl3= obl3 + ' ' + out2[0][i]['text']\n",
        "    for j in  rootobl4:\n",
        "      if  out2[0][i]['id']==j :\n",
        "        obl4= obl4 + ' ' + out2[0][i]['text']\n",
        "    for j in rootnsubj:\n",
        "      if  out2[0][i]['id']==j :\n",
        "        nsubj= nsubj + ' ' + out2[0][i]['text']\n",
        "    for j in rootobj:\n",
        "      if  out2[0][i]['id']==j :\n",
        "        obj= obj + ' ' + out2[0][i]['text']\n",
        "\n",
        "  return nsubj,obj,obl1,obl2,obl3,obl4,verb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JLB_2K-H5dl"
      },
      "outputs": [],
      "source": [
        "def wordcloud_def(text,stop_words):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  \n",
        "  text='' \n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          text=text + ' ' + w\n",
        "  return(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVhlKcqRFgC3"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "def stopwords():\n",
        "  nmz = Normalizer()\n",
        "  stop_words = \"\\n\".join(\n",
        "      sorted(\n",
        "          list(\n",
        "              set(\n",
        "                  [\n",
        "                      nmz.normalize(w) for w in codecs.open('/content/drive/MyDrive/Thesis/persian', encoding='utf-8').read().split('\\n') if w\n",
        "                  ]\n",
        "              )\n",
        "          )\n",
        "      )\n",
        "  )\n",
        "  return(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqyEWiCGlmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "738c5d43-7f8d-4a29-c721-77d3d1166af7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' دانشگاه'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "text=\"در دانشگاه\"\n",
        "stop_words=stopwords()\n",
        "wordcloud_def(text,stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KM4eePfz725"
      },
      "outputs": [],
      "source": [
        "def adverb_type(text):\n",
        "  from dadmatools.models.normalizer import Normalizer\n",
        "  stop_words=stopwords()\n",
        "  text=wordcloud_def(text,stop_words)\n",
        "  normalized_text = normalizer.normalize(text)\n",
        "  #print(\"normalized_text\",normalized_text)\n",
        "  typeAdverbs=''\n",
        "  if normalized_text !='':\n",
        "    typeAdverbs=''\n",
        "    doc=nlp( normalized_text)\n",
        "    #print(doc)\n",
        "    sentences = doc._.sentences\n",
        "    #print(\"sentences::::\",sentences)\n",
        "    for sentence in sentences:\n",
        "      out3=parser(str(sentence))\n",
        "      #print(out3)\n",
        "      #print(\"text111111111111111:::\",out3)\n",
        "      noun=[]\n",
        "      for i in range(len(out3[0])):\n",
        "        if out3[0][i]['pos']=='NOUN' :\n",
        "          noun.append(out3[0][i]['text'])\n",
        "#اگر در جمله اسم وجود نداشت \n",
        "      if not noun:\n",
        "        for i in range(len(out3[0])):\n",
        "          noun.append(out3[0][i]['text'])\n",
        "    #  for sentence in moun:\n",
        "  #       text = sentence\n",
        "      #print(noun)\n",
        "      for token in noun:\n",
        "          #print(str(token))\n",
        "          typeAdverbs=Adverb(str(token))\n",
        "          if typeAdverbs=='T' or typeAdverbs=='P' or typeAdverbs=='M':\n",
        "            #print(\"typeAdverbs\",typeAdverbs)\n",
        "            break\n",
        "  \n",
        "  return typeAdverbs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDvKvF22eyxK"
      },
      "outputs": [],
      "source": [
        "# CREATING THE TABLE\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA.db')\n",
        "#print(\"Opened database successfully\");\n",
        "\n",
        "#conn.execute('''DROP TABLE IF EXISTS knowledge_data1;''')\n",
        "conn.commit()\n",
        "conn.execute('''\n",
        "CREATE TABLE IF NOT EXISTS knowledge_data3(id INTEGER PRIMARY KEY AUTOINCREMENT, sentence text, nsubj text, \n",
        "                      obj text, \n",
        "                      oblTime text, \n",
        "                      oblPlace text, \n",
        "                      oblManner text, \n",
        "                      oblNull text, \n",
        "                      verb text);''')\n",
        "\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "#print(\"Table created successfully\");\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRUG5qmnQj6N"
      },
      "outputs": [],
      "source": [
        "def quintuplet(doc,id,flag):\n",
        " # doc = 'سوره بقره یكصد و بیست آیه دارد و چند ماه پیش از رحلت پیامبر اكرم نازل شده و هیچ یك از آیات آن نسخ نگشته است .'\n",
        "  #print(doc)\n",
        "  out1=parser(doc)\n",
        "  #print(out1)\n",
        "  id1=id\n",
        "  rootid,rootfirst,out2=rootid_(out1)\n",
        "  out2,rootnsubj,rootobj,rootobl1,rootobl2,rootobl3,rootobl4=nsubj_obl_obj(out2,rootfirst)\n",
        "  out1=parser(doc)\n",
        "\n",
        "  nsubj,obj,obl1,obl2,obl3,obl4,verb=pars_text(out2,rootnsubj,rootobl1,rootobl2,rootobl3,rootobl4,rootobj,rootid)\n",
        "  #print('فاعل جمله : ','*****',nsubj,'*****')\n",
        "  #print('مفعول جمله : ','*****',obj,'*****')\n",
        "  #print('قید اول جمله : ','*****',obl1,'*****')\n",
        "  #print('قید دوم جمله : ','*****',obl2,'*****')\n",
        "  #print('قید سوم جمله : ','*****',obl3,'*****')\n",
        "  #print('قید چهارم جمله : ','*****',obl4,'*****')\n",
        "  #print('فعل جمله : ','*****',verb,'*****')\n",
        "\n",
        "  rootnsubj,rootobj,rootobl1,rootobl2,rootobl3,rootobl4,rootid=nsubj_obl_obj_full(out2,rootnsubj,rootobl1,rootobl2,rootobl3,rootobl4,rootobj,rootid)\n",
        "  rootnsubj.sort()\n",
        "  rootobl1.sort()\n",
        "  rootobl2.sort()\n",
        "  rootobl3.sort()\n",
        "  rootobl4.sort()\n",
        "  rootobj.sort()\n",
        "  rootid.sort()\n",
        "  #print(rootnsubj)\n",
        "  #print(rootobj)\n",
        "  #print(rootobl1)\n",
        "  #print(rootobl2)\n",
        "  #print(rootobl3)\n",
        "  #print(rootobl4)\n",
        "  #print(rootid)\n",
        "\n",
        "  out1=parser(doc)\n",
        "  nsubj,obj,obl1,obl2,obl3,obl4,verb=pars_text(out2,rootnsubj,rootobl1,rootobl2,rootobl3,rootobl4,rootobj,rootid)\n",
        "  #print('فاعل جمله : ','*****',nsubj,'*****')\n",
        "  #print('مفعول جمله : ','*****',obj,'*****')\n",
        "  #print('قید اول جمله : ','*****',obl1,'*****')\n",
        "  #print('قید دوم جمله : ','*****',obl2,'*****')\n",
        "  #print('قید سوم جمله : ','*****',obl3,'*****')\n",
        "  #print('قید چهارم جمله : ','*****',obl4,'*****')\n",
        "  #print('فعل جمله : ','*****',verb,'*****')\n",
        "  oblTime =''\n",
        "  oblPlace =''\n",
        "  oblManner =''\n",
        "  oblNull =''\n",
        "  oblNull1=''\n",
        "  typeAdv =''\n",
        "  #Prediction the type of adverbs\n",
        "  \n",
        "  if obl1 !='':\n",
        "    \n",
        "    typeAdv=adverb_type(obl1)\n",
        "    \n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl1\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl1\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl1\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl1\n",
        "      oblNull1=oblNull1 + obl1\n",
        "  \n",
        "#############\n",
        "  if obl2 !='':\n",
        "    #print(\"obl2\",obl2)\n",
        "    typeAdv=adverb_type(obl2)\n",
        "    #print(\"typeAdv,\",typeAdv)\n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl2\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl2\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl2\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl2\n",
        "      oblNull1=oblNull1 + obl2\n",
        "  \n",
        "\n",
        "#############\n",
        "  if obl3 !='':\n",
        "    typeAdv=adverb_type(obl3)\n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl3\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl3\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl3\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl3\n",
        "      oblNull1=oblNull1 +  obl3\n",
        "      \n",
        "#############\n",
        "  if obl4 !='':\n",
        "    typeAdv=adverb_type(obl4)\n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl4\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl4\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl4\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl4\n",
        "      oblNull1=oblNull1  + obl4\n",
        "  # INSERTING VALUES\n",
        "  #print(\"oblPlace\",oblPlace)\n",
        "  if flag=='1':\n",
        "    conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA.db')\n",
        "    cursor = conn.cursor()\n",
        "    sqlite_insert_with_param = \"\"\"INSERT INTO knowledge_data1\n",
        "                      ( sentence, nsubj, obj, oblTime, oblPlace, oblManner, oblNull, verb) \n",
        "                      VALUES ( ?, ?, ?, ?, ?, ?, ?, ?);\"\"\"\n",
        "\n",
        "    data_tuple = (doc, nsubj, obj, oblTime, oblPlace, oblManner, oblNull1, verb)\n",
        "    cursor.execute(sqlite_insert_with_param, data_tuple)\n",
        "    conn.commit()\n",
        "\n",
        "    #print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@222222\",oblNull1)\n",
        "    #print(out1)\n",
        "  else:\n",
        "    return id, doc, nsubj, obj, oblTime, oblPlace, oblManner, oblNull1, verb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa68SRAsgyTm"
      },
      "outputs": [],
      "source": [
        "def Adverb(word):\n",
        "  try:\n",
        "      string=word\n",
        "      str=[]\n",
        "      \n",
        "      str.append(string)\n",
        "      #sim1= embedding.top_nearest([string],20)\n",
        "      sim1=model.wv.most_similar(string,topn=20)\n",
        "      \n",
        "      for index, row in enumerate(sim1):\n",
        "        ##print(sim1[index][0])\n",
        "        str.append(sim1[index][0])\n",
        "    ####################################1\n",
        "      \n",
        "      list_Time=[]\n",
        "      for (num,item) in enumerate(str):\n",
        "          ##print(num,item)\n",
        "          string1 = item   \n",
        "          # opening a text file\n",
        "          file_Time = open(\"/content/drive/MyDrive/Thesis/Adverb_of_Time.txt\", \"r\")\n",
        "          # setting flag and index to 0\n",
        "          flag_Time = 0\n",
        "          index_Time = 0      \n",
        "          # Loop through the file line by line\n",
        "          for line_Time in file_Time:  \n",
        "              index_Time = index_Time +1           \n",
        "              # checking string is present in line or not\n",
        "              if string1 in line_Time:     \n",
        "                flag_Time = 1\n",
        "                break \n",
        "                    \n",
        "          # checking condition for string found or not\n",
        "          if flag_Time == 0: \n",
        "            flag_Time=0\n",
        "          # #print('String', string1 , 'Not Found') \n",
        "          else: \n",
        "            ##print('String', string1, 'Found In Line', index_Time)\n",
        "            list_Time.append(num+1)\n",
        "          # closing text file    \n",
        "          file_Time.close() \n",
        "\n",
        "    # for item in list_Time:\n",
        "          ##print(item)\n",
        "    ##################################2\n",
        "      list_place=[]\n",
        "      for (num,item) in enumerate(str):\n",
        "          ##print(num,item)\n",
        "          string1 = item\n",
        "          file_place = open(\"/content/drive/MyDrive/Thesis/Adverb_of_place.txt\", \"r\")   \n",
        "          flag_place = 0\n",
        "          index_place = 0 \n",
        "          # Loop through the file line by line\n",
        "          for line_place in file_place:  \n",
        "              index_place = index_place +1\n",
        "              # checking string is present in line or not\n",
        "              if string1 in line_place:\n",
        "                flag_place = 1\n",
        "                break   \n",
        "          # checking condition for string found or not\n",
        "          if flag_place == 0: \n",
        "            flag_place=0\n",
        "            ##print('String', string1 , 'Not Found') \n",
        "          else: \n",
        "            ##print('String', string1, 'Found In Line', index_place)\n",
        "            list_place.append(num+1)\n",
        "            \n",
        "          # closing text file    \n",
        "          file_place.close() \n",
        "      #for item in list_place:\n",
        "\n",
        "          ##print(item)\n",
        "    ################################3\n",
        "      list_manner=[]\n",
        "      for (num,item) in enumerate(str):\n",
        "      #   #print(num,item)\n",
        "          string1 = item\n",
        "          file_manner = open(\"/content/drive/MyDrive/Thesis/Adverb_of_manner.txt\", \"r\")   \n",
        "          flag_manner = 0\n",
        "          index_manner = 0\n",
        "          # Loop through the file line by line\n",
        "          for line_manner in file_manner:  \n",
        "              index_manner = index_manner +1\n",
        "              # checking string is present in line or not\n",
        "              if string1 in line_manner:\n",
        "                flag_manner = 1\n",
        "                break   \n",
        "          # checking condition for string found or not\n",
        "          if flag_manner == 0: \n",
        "            flag_manner=0\n",
        "            ##print('String', string1 , 'Not Found') \n",
        "          else: \n",
        "            ##print('String', string1, 'Found In Line', index_manner)\n",
        "            list_manner.append(num+1)\n",
        "          # closing text file    \n",
        "          file_manner.close() \n",
        "    #  for item in list_manner:\n",
        "      #   #print(item)\n",
        "    #################################prediction_type_adverb\n",
        "      typeAdverb=''\n",
        "      if len(list_Time) > len(list_place) and len(list_Time) > len(list_manner):\n",
        "        #print('قید زمان است')\n",
        "        typeAdverb='T'\n",
        "      elif len(list_Time) < len(list_place) and len(list_manner) < len(list_place):\n",
        "        #print('قید مکان است')\n",
        "        typeAdverb='P'\n",
        "      elif len(list_Time) < len(list_manner) and len(list_place) < len(list_manner):\n",
        "        #print('قید حالت است')\n",
        "        typeAdverb='M'\n",
        "      elif len(list_Time)==0 and len(list_place)==0 and len(list_manner)==0:\n",
        "        #print('قید مکان یا زمان یا حالت نیست')\n",
        "        typeAdverb='N'\n",
        "      elif len(list_Time)==0 and len(list_place)==len(list_manner):\n",
        "        for x in range(0, len(list_place)):\n",
        "          if  list_manner[x] > list_place[x]:\n",
        "            #print('قید مکان است')\n",
        "            typeAdverb='P'\n",
        "            break\n",
        "          if list_manner[x] < list_place[x]:\n",
        "            #print('قید حالت است')\n",
        "            typeAdverb='T'\n",
        "            break\n",
        "\n",
        "      elif len(list_place)==0 and len(list_Time)==len(list_manner):\n",
        "        for x in range(0, len(list_Time)):\n",
        "          if  list_manner[x] > list_Time[x]:\n",
        "            #print('قید زمان است')\n",
        "            typeAdverb='P'\n",
        "            break\n",
        "          if list_manner[x] < list_Time[x]:\n",
        "            #print('قید حالت است')\n",
        "            typeAdverb='T'\n",
        "            break\n",
        "\n",
        "      elif len(list_manner)==0 and len(list_Time)==len(list_place):\n",
        "        for x in range(0, len(list_Time)):\n",
        "          if  list_place[x] > list_Time[x]:\n",
        "            #print('قید زمان است')\n",
        "            typeAdverb='P'\n",
        "            break\n",
        "          if list_place[x] < list_Time[x]:\n",
        "            #print('قید مکان است')\n",
        "            typeAdverb='T'\n",
        "            break\n",
        "  except KeyError:\n",
        "      return 'N'\n",
        "  else:  \n",
        "      return typeAdverb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkDF3i5JaMV7"
      },
      "outputs": [],
      "source": [
        "def dadma_first(text,flag):\n",
        "\n",
        "  id=0\n",
        "\n",
        "  text1=[]\n",
        "\n",
        "  text1=sent_tokenize(text)\n",
        "  #for sent in text1:\n",
        "  #print(\"text1,text1\",text)\n",
        "  senn=[]\n",
        "  \n",
        "  for sent in text1:\n",
        "    #print('جمله:::::::',sent)\n",
        "\n",
        "    id=id + 1\n",
        "    quintuplet(sent,id,flag)\n",
        "\n",
        "    doc1 = nlp(sent)\n",
        "    out1 = language.to_json(pips, doc1)\n",
        "    senn=segmentation_verb(out1)\n",
        "    #print(\"sennnnn\",senn)\n",
        "    #print(\"%%%%%%%%%%%%%%%%%5\")\n",
        "    #print(\"%%%%%%%%%%%%%%%%%5\")\n",
        "    #print(\"%%%%%%%%%%%%%%%%%5\")\n",
        "    if senn:\n",
        "      for sen in senn:\n",
        "        #print(sen,'verbbbbbbbbbb')\n",
        "        id=id + 1\n",
        "        quintuplet(sen,id,flag)\n",
        "      #  doc1 = nlp(str(sen))\n",
        "      #  out21 = language.to_json(pips, doc1)\n",
        "      #  #print(out21)\n",
        "\n",
        "    # doc1 = nlp(str(sent))\n",
        "    # out21 = language.to_json(pips, doc1)\n",
        "    # #print(out21)\n",
        "\n",
        "\n",
        "\n",
        "  #print(id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWh3tBlsNnj"
      },
      "source": [
        "**هضم**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkukeG9v5uLn"
      },
      "outputs": [],
      "source": [
        "def segmentation_verb_hazm(txt):\n",
        "\n",
        "  tagger = POSTagger(model='resources/postagger.model')\n",
        "  lemmatizer = Lemmatizer()\n",
        "  parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer)\n",
        "\n",
        "  #نرمالسازی و تمیز کردن جملات\n",
        "\n",
        "  #print(txt)\n",
        "  #تبدیل متن به جملات\n",
        "  text1=sent_tokenize(txt)\n",
        "  #print(text1)\n",
        "  #print('##########################')\n",
        "  list1=[]\n",
        "\n",
        "  sentences = parser.parse(word_tokenize(txt))\n",
        "  Sentence=''\n",
        "  sentn=[]\n",
        "  verb__=[]\n",
        "  list_hazm=[]\n",
        "  list_word=word_tokenize(txt)\n",
        "  for a in parser.parse(word_tokenize(txt)).triples():\n",
        "    aa=a\n",
        "    list_hazm.append(a)\n",
        "    #print(a)\n",
        "  #print('VERB',list_hazm)\n",
        "  \n",
        "  for i in range(len(list_hazm)):\n",
        "    if list_hazm[i][0][1]=='V' or (len(list_hazm)>i+1 and list_hazm[i+1][1]=='SBJ'):\n",
        "      verb__.append(list_hazm[i][0][0])\n",
        "\n",
        "  for i in list_word:\n",
        "    if i not in verb__:\n",
        "      Sentence= Sentence + ' ' + i\n",
        "    else:\n",
        "      Sentence= Sentence + ' ' + i\n",
        "      sentn.append(Sentence)\n",
        "      Sentence=''\n",
        "\n",
        "  if len(sentn)==1:\n",
        "    sentn=[]\n",
        "\n",
        "  return sentn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04UkJGFTsQDL"
      },
      "outputs": [],
      "source": [
        "def hazm_first(text,flag):\n",
        "  id=0\n",
        "\n",
        "  text1=[]\n",
        "  from hazm import sent_tokenize, word_tokenize\n",
        "\n",
        "  text1=sent_tokenize(text)\n",
        "  #for sent in text1:\n",
        "  #print(\"text1,text1\",text)\n",
        "  senn=[]\n",
        "  flag='1'\n",
        "  for sent in text1:\n",
        "    #print('جمله:::::::',sent)\n",
        "\n",
        "    id=id + 1\n",
        "    quintuplethazm(sent,id,flag)\n",
        "\n",
        "    senn=segmentation_verb_hazm(sent)\n",
        "    #print(\"sennnnn\",senn)\n",
        "    #print(\"%%%%%%%%%%%%%%%%%5\")\n",
        "    #print(\"%%%%%%%%%%%%%%%%%5\")\n",
        "    #print(\"%%%%%%%%%%%%%%%%%5\")\n",
        "    if senn:\n",
        "      for sen in senn:\n",
        "        #print(sen,'verbbbbbbbbbb')\n",
        "        id=id + 1\n",
        "        quintuplethazm(sen,id,flag)\n",
        "      #  doc1 = nlp(str(sen))\n",
        "      #  out21 = language.to_json(pips, doc1)\n",
        "      #  #print(out21)\n",
        "\n",
        "    # doc1 = nlp(str(sent))\n",
        "    # out21 = language.to_json(pips, doc1)\n",
        "    # #print(out21)\n",
        "\n",
        "\n",
        "\n",
        "  #print(id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9na7udkq2C1"
      },
      "outputs": [],
      "source": [
        "def hazmparse(txt):  \n",
        "  try:  \n",
        "      tagger = POSTagger(model='resources/postagger.model')\n",
        "      lemmatizer = Lemmatizer()\n",
        "      parser = DependencyParser(tagger=tagger, lemmatizer=lemmatizer)\n",
        "\n",
        "      #نرمالسازی و تمیز کردن جملات\n",
        "\n",
        "      #print(txt)\n",
        "      #تبدیل متن به جملات\n",
        "      text1=sent_tokenize(txt)\n",
        "      #print(text1)\n",
        "      #print('##########################')\n",
        "      list1=[]\n",
        "\n",
        "      sentences = parser.parse(word_tokenize(txt))\n",
        "      #print(sentences.tree())\n",
        "\n",
        "      #print('##########################')\n",
        "      SBJ=''\n",
        "      OBJ=''\n",
        "      VERB=''\n",
        "\n",
        "      list_hazm=[]\n",
        "      for a in parser.parse(word_tokenize(txt)).triples():\n",
        "        aa=a\n",
        "        list_hazm.append(a)\n",
        "        #print(a)\n",
        "      #print('VERB',list_hazm)\n",
        "      \n",
        "      \n",
        "\n",
        "      ADV1=''\n",
        "      ADV2=''\n",
        "      ADV3=''\n",
        "      list_adv1=[]\n",
        "      list_adv2=[]\n",
        "      list_adv3=[]\n",
        "      list_sbj=[]\n",
        "      list_obj=[]\n",
        "      ADV1_clear=1\n",
        "      ADV2_clear=1\n",
        "      ADV3_clear=1\n",
        "      List_ADV = ['ADV','VPP']\n",
        "      for i in range(len(list_hazm)):\n",
        "        if list_hazm[i][0][1]=='V':\n",
        "          VERB=list_hazm[i][0][0]\n",
        "\n",
        "\n",
        "      for i in range(len(list_hazm)):\n",
        "    #    #print(list_hazm[i][1])\n",
        "    #    if list[i][1]=='SBJ':\n",
        "    #      SBJ= list[i][2][0]\n",
        "    #      if len(list)>i+1 and list[i+1][2][1]not in List_ADV and list[i+1][0][0]==list[i][2][0]:\n",
        "    #        #print(list[i+1][2][0])\n",
        "    #        SBJ= SBJ + ' '+ list[i+1][2][0]\n",
        "    #        if len(list)>i+2 and list[i+2][0][0]==list[i+1][2][0]:\n",
        "    #          SBJ= SBJ + ' '+ list[i+2][2][0]\n",
        "    #      if len(list)>i+1 and list[i+1][2][1]in List_ADV:\n",
        "    #        ADV1= list[i+1][2][0]\n",
        "    #        if len(list)>i+2 and list[i+2][0][0]==list[i+1][2][0]:\n",
        "    #          ADV1= ADV1 + ' '+ list[i+2][2][0]\n",
        "    #          if len(list)>i+3 and list[i+3][0][0]==list[i+2][2][0]:\n",
        "    #            ADV1= ADV1 + ' '+ list[i+3][2][0]\n",
        "    #      continue\n",
        "        if list_hazm[i][1]=='SBJ' or list_hazm[i][0][0] in list_sbj:\n",
        "          list_sbj.append(list_hazm[i][2][0])\n",
        "          SBJ= SBJ + ' '+ list_hazm[i][2][0]\n",
        "          if len(list_hazm)==i+1 or list_hazm[i+1][0][1]=='V':\n",
        "            list_sbj.clear()\n",
        "          continue\n",
        "\n",
        "        elif list_hazm[i][1]=='OBJ' or list_hazm[i][0][0] in list_obj:\n",
        "          list_obj.append(list_hazm[i][2][0])\n",
        "          OBJ= OBJ + ' '+ list_hazm[i][2][0]\n",
        "          if len(list_hazm)==i+1 or list_hazm[i+1][0][1]=='V':\n",
        "            list_obj.clear()\n",
        "          continue\n",
        "          \n",
        "        elif  ADV1_clear==1  and (list_hazm[i][1] in List_ADV or list_hazm[i][0][0] in list_adv1):\n",
        "          list_adv1.append(list_hazm[i][2][0])\n",
        "          ADV1= ADV1 + ' '+ list_hazm[i][2][0] \n",
        "          if len(list_hazm)==i+1 or list_hazm[i+1][0][1]=='V':\n",
        "            list_adv1.clear()\n",
        "            ADV1_clear=0                                       \n",
        "          continue\n",
        "\n",
        "        elif  ADV2_clear==1  and (list_hazm[i][1] in List_ADV or list_hazm[i][0][0] in list_adv2):\n",
        "          list_adv2.append(list_hazm[i][2][0])\n",
        "          ADV2= ADV2 + ' '+ list_hazm[i][2][0] \n",
        "          if len(list_hazm)==i+1 or list_hazm[i+1][0][1]=='V':\n",
        "            list_adv2.clear()  \n",
        "            ADV2_clear=0                                      \n",
        "          continue\n",
        "\n",
        "        elif  ADV3_clear==1 and (list_hazm[i][1] in List_ADV or list_hazm[i][0][0] in list_adv3):\n",
        "          list_adv3.append(list_hazm[i][2][0])\n",
        "          ADV3= ADV3 + ' '+ list_hazm[i][2][0] \n",
        "          if len(list_hazm)==i+1 or list_hazm[i+1][0][1]=='V':\n",
        "            list_adv3.clear()  \n",
        "            ADV3_clear=0                                      \n",
        "          continue\n",
        "\n",
        "        else:\n",
        "          if list_hazm[i][0][0]==VERB and list_hazm[i][2][1]!='CONJ':\n",
        "            #print(list_hazm[i][2][0])\n",
        "            VERB=list_hazm[i][2][0] + ' ' + VERB\n",
        "\n",
        "\n",
        "      ADV4=''\n",
        "      #print('SBJ: ',SBJ)\n",
        "      #print('OBJ: ',OBJ)\n",
        "      #print('ADV1: ',ADV1)\n",
        "      #print('ADV2: ',ADV2)\n",
        "      #print('ADV3: ',ADV3)\n",
        "      #print('VERB: ',VERB)\n",
        "      \n",
        "\n",
        "  except IndexError:\n",
        "      return '','','','','','',''\n",
        "  else:  \n",
        "      return SBJ,OBJ,ADV1,ADV2,ADV3,ADV4,VERB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcXDmKfkrW5T"
      },
      "outputs": [],
      "source": [
        "def quintuplethazm(doc,id,flag):\n",
        " # doc = 'سوره بقره یكصد و بیست آیه دارد و چند ماه پیش از رحلت پیامبر اكرم نازل شده و هیچ یك از آیات آن نسخ نگشته است .'\n",
        "  #print(doc)\n",
        " \n",
        "  id1=id\n",
        "\n",
        "  nsubj,obj,obl1,obl2,obl3,obl4,verb=hazmparse(doc)\n",
        "  #print('فاعل جمله : ','*****',nsubj,'*****')\n",
        "  #print('مفعول جمله : ','*****',obj,'*****')\n",
        "  #print('قید اول جمله : ','*****',obl1,'*****')\n",
        "  #print('قید دوم جمله : ','*****',obl2,'*****')\n",
        "  #print('قید سوم جمله : ','*****',obl3,'*****')\n",
        "  #print('قید چهارم جمله : ','*****',obl4,'*****')\n",
        "  #print('فعل جمله : ','*****',verb,'*****')\n",
        "  #print('hazm*********************************************hazm')\n",
        "  oblTime =''\n",
        "  oblPlace =''\n",
        "  oblManner =''\n",
        "  oblNull =''\n",
        "  oblNull1=''\n",
        "  typeAdv =''\n",
        "  #Prediction the type of adverbs\n",
        "  \n",
        "  if obl1 !='':\n",
        "    \n",
        "    typeAdv=adverb_type(obl1)\n",
        "    \n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl1\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl1\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl1\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl1\n",
        "      oblNull1=oblNull1 + obl1\n",
        "  \n",
        "#############\n",
        "  if obl2 !='':\n",
        "    #print(\"obl2\",obl2)\n",
        "    typeAdv=adverb_type(obl2)\n",
        "    #print(\"typeAdv,\",typeAdv)\n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl2\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl2\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl2\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl2\n",
        "      oblNull1=oblNull1 + obl2\n",
        "  \n",
        "\n",
        "#############\n",
        "  if obl3 !='':\n",
        "    typeAdv=adverb_type(obl3)\n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl3\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl3\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl3\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl3\n",
        "      oblNull1=oblNull1 +  obl3\n",
        "      \n",
        "#############\n",
        "  if obl4 !='':\n",
        "    typeAdv=adverb_type(obl4)\n",
        "    if typeAdv=='T' and oblTime=='':\n",
        "      oblTime=obl4\n",
        "    elif typeAdv=='P' and oblPlace=='':\n",
        "      oblPlace=obl4\n",
        "    elif typeAdv=='M' and oblManner=='':\n",
        "      oblManner=obl4\n",
        "    elif typeAdv=='N' :\n",
        "      oblNull=obl4\n",
        "      oblNull1=oblNull1  + obl4\n",
        "  # INSERTING VALUES\n",
        "  #print(\"oblPlace\",oblPlace)\n",
        "  if flag=='1':\n",
        "    conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA.db')\n",
        "    cursor = conn.cursor()\n",
        "    sqlite_insert_with_param = \"\"\"INSERT INTO knowledge_data1\n",
        "                      (sentence, nsubj, obj, oblTime, oblPlace, oblManner, oblNull, verb) \n",
        "                      VALUES (?, ?, ?, ?, ?, ?, ?, ?);\"\"\"\n",
        "\n",
        "    data_tuple = (doc, nsubj, obj, oblTime, oblPlace, oblManner, oblNull1, verb)\n",
        "    cursor.execute(sqlite_insert_with_param, data_tuple)\n",
        "    conn.commit()\n",
        "\n",
        "    #print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@222222\",oblNull1)\n",
        "    #print(out1)\n",
        "  else:\n",
        "    return id, doc, nsubj, obj, oblTime, oblPlace, oblManner, oblNull1, verb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gla8VrWIwEoI"
      },
      "outputs": [],
      "source": [
        "from hazm import sent_tokenize, word_tokenize\n",
        "text=\"\"\"همۀ مردان و زنان باایمان و هر مؤمنی را هم که وارد خانه‌ام می‌شود بیامرز\"\"\"\n",
        "text= text.replace(\"\\u200c\", \" \" )\n",
        "normalizer = Normalizer()\n",
        "text = normalizer.normalize(text)\n",
        "flag='1'\n",
        "#dadma_first(text,flag)\n",
        "#print('dadma_first_dadma_first****************************')\n",
        "hazm_first(text,flag)\n",
        "#print('hazm_first_hazm_first****************************')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr5s-EbZt2DR"
      },
      "outputs": [],
      "source": [
        "conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA.db')\n",
        "\n",
        "cursor = conn.execute(''' SELECT id, sentence, nsubj, obj, oblTime, oblPlace, oblManner, oblNull, verb\n",
        "                          FROM knowledge_data_full;''')\n",
        "\n",
        "for row in cursor:\n",
        "  #print(row[2])\n",
        "  print(row)\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqE77KnCH-lB"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and \n",
        "# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\n",
        "#model = SentenceTransformer('HooshvareLab/bert-fa-zwnj-base')\n",
        "#model = SentenceTransformer('HooshvareLab/bert-fa-base-uncased-clf-digimag')\n",
        "model1 = SentenceTransformer('HooshvareLab/bert-fa-base-uncased-clf-persiannews')\n",
        "#model1 = SentenceTransformer('HooshvareLab/bert-fa-zwnj-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyS0rrHNIHZ_"
      },
      "outputs": [],
      "source": [
        "#from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#from sentence-transformers import SentenceTransformer\n",
        "import scipy\n",
        "import csv\n",
        "\n",
        "#model = SentenceTransformer('HooshvareLab/bert-fa-zwnj-base')\n",
        "#model = SentenceTransformer('HooshvareLab/bert-fa-base-uncased-clf-digimag')\n",
        "#model1 = SentenceTransformer('HooshvareLab/bert-fa-base-uncased-clf-persiannews')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KX_JpziuTaK"
      },
      "outputs": [],
      "source": [
        "conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA1.db')\n",
        "cursor = conn.cursor()\n",
        "sql_delete_query = \"\"\"INSERT INTO knowledge_data_full\n",
        "                      (sentence, nsubj, obj, oblTime, oblPlace, oblManner, oblNull, verb) SELECT sentence, nsubj, obj, oblTime, oblPlace, oblManner, oblNull, verb from knowledge_data1;\"\"\"\n",
        "\n",
        "cursor.execute(sql_delete_query)\n",
        "#print(\"Record deleted successfully \")\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CaRiTOqzsQM"
      },
      "outputs": [],
      "source": [
        "conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA.db')\n",
        "\n",
        "cursor = conn.execute(''' SELECT id, sentence\n",
        "                          FROM knowledge_data_full;''')\n",
        "list_sent=[]\n",
        "for row in cursor:\n",
        "  #print(row[1])\n",
        "  print(row[0])\n",
        "  list_sent.append(row[1])\n",
        "  \n",
        "sentence_embeddings = model1.encode(list_sent)\n",
        "#sen_embedding2.shape\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKuzFja9o7jk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def embedding_Comparison(item1,item2):\n",
        "  sen_embedding3 = model1.encode(item1)\n",
        "  sen_embedding4 = model1.encode(item2)\n",
        "  sen_embedding3.shape\n",
        "  sen_embedding4.shape\n",
        "\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  #let's calculate cosine similarity for sentence 0:\n",
        "  Comparison=cosine_similarity(\n",
        "      [sen_embedding3],\n",
        "      [sen_embedding4]\n",
        "  )\n",
        "  return(Comparison[0][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCstPMZsEffq"
      },
      "outputs": [],
      "source": [
        "def sort1(tex,sentence):\n",
        "  tex=word_tokenize (tex)\n",
        "  sentence=word_tokenize (sentence)\n",
        "  list_index=[]\n",
        "  for i in range(len(tex)):\n",
        "    list_index.append(sentence.index(tex[i]))\n",
        "    #print(tex[i])\n",
        "    #print(sentence.index(tex[i]))\n",
        "  list_index.sort()\n",
        "  txt=''\n",
        "  for i in range(list_index[0],list_index[0]+len(list_index)):\n",
        "    txt=txt + ' ' + sentence[i]\n",
        "    #print(sentence[i])\n",
        "  return txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFS9W1vr2fUQ"
      },
      "source": [
        "# **پردازش پرسش**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yM4LFoLtTV3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoahXjM52dO8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title پرسش با پایگاه داده سئوالات\n",
        "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@222\n",
        "import csv\n",
        "file = open('/content/drive/MyDrive/Thesis/out21-1.csv')\n",
        "\n",
        "type(file)\n",
        "csvreader = csv.reader(file)\n",
        "header = []\n",
        "header = next(csvreader)\n",
        "header\n",
        "rows = []\n",
        "\n",
        "\n",
        "for row in csvreader:\n",
        "        rows.append(row)\n",
        "rows\n",
        "\n",
        "def Comp(row,type_Q,list_Question,nsubj_com,obj_com,oblTime_com,oblPlace_com,oblManner_com,oblNull_com,verb_com):\n",
        "  Result_sum=0\n",
        "  Result_Ans=''\n",
        "  N=0\n",
        "  if list_Question[2]=='':\n",
        "    nsubj_com=0\n",
        "  else: N=N+1 \n",
        "  if list_Question[3]=='':\n",
        "    obj_com=0 \n",
        "  else: N=N+1 \n",
        "  if list_Question[4]=='':\n",
        "    oblTime_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[5]=='':\n",
        "    oblPlace_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[6]=='':\n",
        "    oblManner_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[7]=='':\n",
        "    oblNull_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[8]=='':\n",
        "    verb_com=0\n",
        "  else: N=N+1  \n",
        "\n",
        "  N=N-1\n",
        "  if type_Q == 'nsubj':\n",
        "      #print('سئوال در خصوص فاعل می باشد')\n",
        "      if row[2]:\n",
        "        Result_sum = obj_com + oblTime_com + oblPlace_com + oblManner_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[2]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "        \n",
        "  elif type_Q == \"obj\":\n",
        "      #print('سئوال در خصوص مفعول می باشد')\n",
        "      if row[3]:\n",
        "        Result_sum = nsubj_com + oblTime_com + oblPlace_com + oblManner_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[3]\n",
        "      else:\n",
        "        Result_sum=0 \n",
        "\n",
        "  elif type_Q == \"oblTime\":\n",
        "      #print('سئوال در خصوص قید زمان می باشد' )\n",
        "      if row[4]:\n",
        "        Result_sum = nsubj_com + obj_com + oblPlace_com + oblManner_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[4]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "\n",
        "  elif type_Q == \"oblPlace\":\n",
        "      #print('سئوال در خصوص قید مکان می باشد')\n",
        "      if row[5]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com  + oblManner_com + oblNull_com + verb_com \n",
        "        #print(nsubj_com , obj_com , oblTime_com  , oblManner_com , oblNull_com , verb_com )\n",
        "        #print('olace:::::::::::::::::',Result_sum )\n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[5]\n",
        "\n",
        "      else:\n",
        "        Result_sum=0   \n",
        "\n",
        "  elif type_Q == \"oblManner\":\n",
        "      #print('سئوال در خصوص قید ... می باشد')\n",
        "      if row[6]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com + oblPlace_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[6]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "\n",
        "  elif type_Q == \"verb\":\n",
        "      #print('سئوال در خصوص فعل می باشد')\n",
        "      if row[8]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com + oblPlace_com + oblManner_com + oblNull_com  \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[8]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "\n",
        "  else:\n",
        "      #print(\"Please choose correct answer\")\n",
        "      if row[7]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com + oblPlace_com + oblManner_com + verb_com\n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[7] \n",
        "      else:\n",
        "        Result_sum=0\n",
        "  return Result_sum,Result_Ans\n",
        "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@2\n",
        "\n",
        "with open('/content/drive/MyDrive/Thesis/output1.csv', 'w', newline='') as file1, open('/content/drive/MyDrive/Thesis/output2.csv', 'w', newline='') as file2, open('/content/drive/MyDrive/Thesis/output3.csv', 'w', newline='') as file3:\n",
        "    writer1 = csv.writer(file1)\n",
        "    writer2 = csv.writer(file2)\n",
        "    writer3 = csv.writer(file3)\n",
        "    writer1.writerow([\"SN\", \"Question\", \"Question_type\"])\n",
        "    writer2.writerow([\"SN\", \"Question\", \"Answer\"])\n",
        "    writer3.writerow([\"SN\", \"Question\", \"Candidate\"])\n",
        "    for row in range(len(rows)) :\n",
        "      text=rows[row][0]\n",
        "      text= text.replace(\"\\u200c\", \" \" )\n",
        "      normalizer = Normalizer()\n",
        "      text = normalizer.normalize(text)\n",
        "      text1=sent_tokenize(text)\n",
        "      #for sent in text1:\n",
        "      #print(\"text1,text1\",text)\n",
        "      senn=[]\n",
        "      flag='0'\n",
        "      for sent in text1:\n",
        "        #print('جمله:::::::',sent)\n",
        " \n",
        "        id2, doc2, nsubj2, obj2, oblTime2, oblPlace2, oblManner2, oblNull12, verb2=quintuplethazm(sent,id,flag)\n",
        "        list_Question=(id2, doc2, nsubj2, obj2, oblTime2, oblPlace2, oblManner2, oblNull12, verb2)\n",
        "        #id, doc, nsubj, obj, oblTime, oblPlace, oblManner, oblNull1, verb=quintuplet(sent,id,flag)\n",
        "        #print(id2, doc2, nsubj2, obj2, oblTime2, oblPlace2, oblManner2, oblNull12, verb2)\n",
        "\n",
        "      #print(doc2, nsubj2, obj2, oblTime2, oblPlace2, oblManner2, oblNull12, verb2)\n",
        "      question_list=['چه‌ جور','چیست','مکانی','چه سان','چگونه','چطور','چه\\u200cطور','چه\\u200cسان','چه اندازه','چه\\u200cاندازه','چند','چه مقدار','چه\\u200cقدر','چه\\u200cمقدار','چندمی','چندم','چندمین','کجا','کدامی','چقدر','کدام','چه زمانی','چه وقت','چه','چه\\u200cکسی' , 'چه\\u200cفردی', 'چه\\u200cشخصی', 'چه کسی' ,'چه\\u200cکسانی' , 'چه\\u200cافرادی', 'چه\\u200cاشخاصی', 'چه کسانی','چه\\u200cچیزی' , 'چه\\u200cوسیله\\u200cای', 'چه\\u200cشی\\u200cای', 'چه چیزی' , 'چه وسیله ای', 'چه شی ای']\n",
        "      type_Q=''\n",
        "      Question_type=''\n",
        "     \n",
        "      if bool([ele for ele in question_list if(ele in nsubj2)]):\n",
        "        Question_type='سئوال در خصوص فاعل می باشد' \n",
        "        type_Q='nsubj'\n",
        "   \n",
        "      elif bool([ele for ele in question_list if(ele in obj2)]):\n",
        "        Question_type='سئوال در خصوص مفعول می باشد' \n",
        "        type_Q='obj'\n",
        "   \n",
        "      elif bool([ele for ele in question_list if(ele in oblTime2)]):\n",
        "        Question_type='سئوال در خصوص قید زمان می باشد' \n",
        "        type_Q='oblTime'\n",
        "      elif bool([ele for ele in question_list if(ele in oblPlace2)]):\n",
        "        Question_type='سئوال در خصوص قید مکان می باشد' \n",
        "        type_Q='oblPlace'\n",
        "      elif bool([ele for ele in question_list if(ele in oblManner2)]):\n",
        "        Question_type='سئوال در خصوص قید حالت می باشد' \n",
        "        type_Q='oblManner'\n",
        "      elif bool([ele for ele in question_list if(ele in oblNull12)]):\n",
        "        Question_type='سئوال در خصوص قید ... می باشد' \n",
        "        type_Q='oblNull1'\n",
        "     \n",
        "      elif bool([ele for ele in question_list if(ele in verb2)]):\n",
        "        Question_type='سئوال در خصوص فعل می باشد' \n",
        "        type_Q='verb'\n",
        "\n",
        "      #print(Question_type)\n",
        "      #print(type_Q)\n",
        "\n",
        "\n",
        "      writer1.writerow([row, rows[row][0], Question_type])\n",
        "\n",
        "\n",
        "      conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA.db')\n",
        "\n",
        "      sen_embedding = model1.encode(text)\n",
        "      #sen_embedding.shape\n",
        "\n",
        "      list_id=[]\n",
        "      \n",
        "\n",
        "      # code adapted from https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\n",
        "\n",
        "      #text = 'سئوال را بپورسید ؟' #@param {type: 'string'}\n",
        "\n",
        "      queries = [text]\n",
        "      query_embeddings = model1.encode(queries)\n",
        "\n",
        "      # Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
        "      number_top_matches = 5 \n",
        "\n",
        "      #print(\"Semantic Search Results\")\n",
        "\n",
        "      for query, query_embedding in zip(queries, query_embeddings):\n",
        "          distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
        "\n",
        "          results = zip(range(len(distances)), distances)\n",
        "          results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "          #print(\"\\n\\n======================\\n\\n\")\n",
        "          #print(\"Query:\", query)\n",
        "          #print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "          for idx, distance in results[0:number_top_matches]:\n",
        "            # #print(idx,sentences[idx].strip(), \"(Cosine Score: %.5f)\" % (1-distance))\n",
        "              list_id.append(idx+1)\n",
        "\n",
        "\n",
        "      #print('list_id:::::::::::::::',list_id)\n",
        "      Result_list=[]\n",
        "      Result_Answer=[]\n",
        "      Result_Sent=[]\n",
        "      Result_sum=0\n",
        "      for i in list_id:\n",
        "        #print('iiiiiiiiiiiiiiii:::',i)\n",
        "        cursor = conn.execute(\"SELECT id, sentence, nsubj, obj, oblTime, oblPlace, oblManner, oblNull, verb FROM knowledge_data_full where id=?\",(i,))\n",
        "        \n",
        "\n",
        "        for row1 in cursor:\n",
        "          #print('################',row)\n",
        "          #print('id',row[0])\n",
        "          #print('sentence',row[1])\n",
        "          #print('nsubj',row[2])\n",
        "          #print('obj',row[3])\n",
        "          #print('oblTime',row[4])\n",
        "          #print('oblPlace',row[5])\n",
        "          #print('oblManner',row[6])\n",
        "          #print('oblNull',row[7])\n",
        "          #print('verb',row[8])\n",
        "\n",
        "          if list_Question[2]!='' and row1[2]:\n",
        "            nsubj_com=embedding_Comparison(list_Question[2],row1[2])\n",
        "          else: nsubj_com=0 \n",
        "          if list_Question[3]!='' and row1[3]:\n",
        "            obj_com=embedding_Comparison(list_Question[3],row1[3])\n",
        "          else: obj_com=0 \n",
        "          if list_Question[4]!='' and row1[4]:\n",
        "            oblTime_com=embedding_Comparison(list_Question[4],row1[4])\n",
        "          else: oblTime_com=0 \n",
        "          if list_Question[5]!='' and row1[5]:\n",
        "            oblPlace_com=embedding_Comparison(list_Question[5],row1[5])\n",
        "          else: oblPlace_com=0 \n",
        "          if list_Question[6]!='' and row1[6]:\n",
        "            #print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$4444444')\n",
        "            oblManner_com=embedding_Comparison(list_Question[6],row1[6])\n",
        "          else: oblManner_com=0 \n",
        "          if list_Question[7]!='' and row1[7]:\n",
        "            oblNull_com=embedding_Comparison(list_Question[7],row1[7])\n",
        "          else: oblNull_com=0 \n",
        "          if list_Question[8]!='' and row1[8]:\n",
        "            verb_com=embedding_Comparison(list_Question[8],row1[8])\n",
        "          else: verb_com=0 \n",
        "\n",
        "          Result_sum,Result_Ans=Comp(row1,type_Q,list_Question,nsubj_com,obj_com,oblTime_com,oblPlace_com,oblManner_com,oblNull_com,verb_com)\n",
        "          #print(\"Result_sum : \", Result_sum)\n",
        "          #print(\"Result_Ans : \", Result_Ans)\n",
        "          Result_Sent\n",
        "        Result_list.append(Result_sum)\n",
        "        Result_Answer.append(Result_Ans)\n",
        "        Result_Sent.append(row1[1])\n",
        "\n",
        "      conn.close()\n",
        "\n",
        "      #print(Question_type)\n",
        "      #print(type_Q)\n",
        "      #print(Result_sum)\n",
        "\n",
        "      Result_list1=[]\n",
        "      for res in Result_list:\n",
        "        Result_list1.append(res)\n",
        "      Result_list1.sort()\n",
        "      #print(\"Largest element 1 is:\", Result_list1[-1],\">> Result Answer 1:\",Result_Answer[Result_list.index(Result_list1[-1])],\" >> sentence Answer 1:\",Result_Sent[Result_list.index(Result_list1[-1])])\n",
        "      #print(\"Largest element 2 is:\", Result_list1[-2],\">> Result Answer 2:\",Result_Answer[Result_list.index(Result_list1[-2])],\" >> sentence Answer 2:\",Result_Sent[Result_list.index(Result_list1[-2])])\n",
        "      #print(\"Largest element 3 is:\", Result_list1[-3],\">> Result Answer 3:\",Result_Answer[Result_list.index(Result_list1[-3])],\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-3])])\n",
        "      #print(\"Largest element 3 is:\", Result_list1[-4],\">> Result Answer 3:\",Result_Answer[Result_list.index(Result_list1[-4])],\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-4])])\n",
        "      #print(\"Largest element 3 is:\", Result_list1[-5],\">> Result Answer 3:\",Result_Answer[Result_list.index(Result_list1[-5])],\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-5])])\n",
        "\n",
        "      #print(\"Largest element is:\", max(Result_list))\n",
        "\n",
        "      #print(rows[row][0])\n",
        "      \n",
        "    \n",
        "      #print(\"Largest element 1 is:\", Result_list1[-1],\">> Result Answer 1:\",wordcloud_def(Result_Answer[Result_list.index(Result_list1[-1])],stop_words)\n",
        "      #,\" >> sentence Answer 1:\",Result_Sent[Result_list.index(Result_list1[-1])])\n",
        "      #print(\"Largest element 2 is:\", Result_list1[-2],\">> Result Answer 2:\",wordcloud_def(Result_Answer[Result_list.index(Result_list1[-2])],stop_words)\n",
        "      #,\" >> sentence Answer 2:\",Result_Sent[Result_list.index(Result_list1[-2])])\n",
        "      #print(\"Largest element 3 is:\", Result_list1[-3],\">> Result Answer 3:\",wordcloud_def(Result_Answer[Result_list.index(Result_list1[-3])],stop_words)\n",
        "      #,\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-3])])\n",
        "      \n",
        "      #print(row, rows[row][0])\n",
        "      if Result_list1[-1]!=0:\n",
        "        writer2.writerow([row, rows[row][0], Result_Answer[Result_list.index(Result_list1[-1])]])\n",
        "      else:\n",
        "        writer2.writerow([row, rows[row][0], \"Not_Result\"])\n",
        "      if Result_list1[-1]!=0:\n",
        "        writer3.writerow([row, rows[row][0], Result_Sent[Result_list.index(Result_list1[-1])]])\n",
        "      if Result_list1[-2]!=0:\n",
        "        writer3.writerow([row, rows[row][0], Result_Sent[Result_list.index(Result_list1[-2])]])\n",
        "      if Result_list1[-3]!=0:\n",
        "        writer3.writerow([row, rows[row][0], Result_Sent[Result_list.index(Result_list1[-3])]])\n",
        "      if Result_list1[-4]!=0:\n",
        "        writer3.writerow([row, rows[row][0], Result_Sent[Result_list.index(Result_list1[-4])]])\n",
        "      if Result_list1[-5]!=0:\n",
        "        writer3.writerow([row, rows[row][0], Result_Sent[Result_list.index(Result_list1[-5])]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ILRRw8ryS81",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title پاسخ به پرسش کاربر\n",
        "\n",
        "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@222\n",
        "\n",
        "def Comp(row,type_Q,list_Question,nsubj_com,obj_com,oblTime_com,oblPlace_com,oblManner_com,oblNull_com,verb_com):\n",
        "  Result_sum=0\n",
        "  Result_Ans=''\n",
        "  N=0\n",
        "  if list_Question[2]=='':\n",
        "    nsubj_com=0\n",
        "  else: N=N+1 \n",
        "  if list_Question[3]=='':\n",
        "    obj_com=0 \n",
        "  else: N=N+1 \n",
        "  if list_Question[4]=='':\n",
        "    oblTime_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[5]=='':\n",
        "    oblPlace_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[6]=='':\n",
        "    oblManner_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[7]=='':\n",
        "    oblNull_com=0\n",
        "  else: N=N+1  \n",
        "  if list_Question[8]=='':\n",
        "    verb_com=0\n",
        "  else: N=N+1  \n",
        "\n",
        "  N=N-1\n",
        "  if type_Q == 'nsubj':\n",
        "      #print('سئوال در خصوص فاعل می باشد')\n",
        "      if row[2]:\n",
        "        Result_sum = obj_com + oblTime_com + oblPlace_com + oblManner_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[2]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "        \n",
        "  elif type_Q == \"obj\":\n",
        "      #print('سئوال در خصوص مفعول می باشد')\n",
        "      if row[3]:\n",
        "        Result_sum = nsubj_com + oblTime_com + oblPlace_com + oblManner_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[3]\n",
        "      else:\n",
        "        Result_sum=0 \n",
        "\n",
        "  elif type_Q == \"oblTime\":\n",
        "      #print('سئوال در خصوص قید زمان می باشد' )\n",
        "      if row[4]:\n",
        "        Result_sum = nsubj_com + obj_com + oblPlace_com + oblManner_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[4]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "\n",
        "  elif type_Q == \"oblPlace\":\n",
        "      #print('سئوال در خصوص قید مکان می باشد')\n",
        "      if row[5]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com  + oblManner_com + oblNull_com + verb_com \n",
        "        #print(nsubj_com , obj_com , oblTime_com  , oblManner_com , oblNull_com , verb_com )\n",
        "        #print('olace:::::::::::::::::',Result_sum )\n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[5]\n",
        "\n",
        "      else:\n",
        "        Result_sum=0   \n",
        "\n",
        "  elif type_Q == \"oblManner\":\n",
        "      #print('سئوال در خصوص قید ... می باشد')\n",
        "      if row[6]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com + oblPlace_com + oblNull_com + verb_com \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[6]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "\n",
        "  elif type_Q == \"verb\":\n",
        "      #print('سئوال در خصوص فعل می باشد')\n",
        "      if row[8]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com + oblPlace_com + oblManner_com + oblNull_com  \n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[8]\n",
        "      else:\n",
        "        Result_sum=0\n",
        "\n",
        "  else:\n",
        "      #print(\"Please choose correct answer\")\n",
        "      if row[7]:\n",
        "        Result_sum = nsubj_com + obj_com + oblTime_com + oblPlace_com + oblManner_com + verb_com\n",
        "        Result_sum= Result_sum/N\n",
        "        Result_Ans=row[7] \n",
        "      else:\n",
        "        Result_sum=0\n",
        "  return Result_sum,Result_Ans\n",
        "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@2\n",
        "\n",
        "\n",
        "text1=[]\n",
        "from hazm import sent_tokenize, word_tokenize\n",
        "Text_Question = \"\\u062E\\u062F\\u0627\\u0648\\u0646\\u062F \\u0686\\u0647 \\u06A9\\u0633\\u0627\\u0646\\u06CC \\u0631\\u0627 \\u062F\\u0631 \\u062F\\u0631\\u0648\\u0646 \\u0632\\u0645\\u06CC\\u0646 \\u0642\\u0631\\u0627\\u0631 \\u0645\\u06CC\\u062F\\u0647\\u062F\" #@param {type: 'string'}\n",
        "Text_Question= Text_Question.replace(\"\\u200c\", \" \" )\n",
        "\n",
        "normalizer = Normalizer()\n",
        "Text_Question = normalizer.normalize(Text_Question)\n",
        "text1=sent_tokenize(Text_Question)\n",
        "#for sent in text1:\n",
        "print(\"text1,text1\",Text_Question)\n",
        "senn=[]\n",
        "flag='0'\n",
        "for sent in text1:\n",
        "  print('جمله:::::::',sent)\n",
        "  \n",
        "  id2, doc2, nsubj2, obj2, oblTime2, oblPlace2, oblManner2, oblNull12, verb2=quintuplethazm(sent,id,flag)\n",
        "  list_Question=(id2, doc2, nsubj2, obj2, oblTime2, oblPlace2, oblManner2, oblNull12, verb2)\n",
        "  #id, doc, nsubj, obj, oblTime, oblPlace, oblManner, oblNull1, verb=quintuplet(sent,id,flag)\n",
        "  #print(id2, doc2, nsubj2, obj2, oblTime2, oblPlace2, oblManner2, oblNull12, verb2)\n",
        "\n",
        "\n",
        "\n",
        "print(\"doc2:\",doc2, \"nsubj2\",nsubj2, \"obj2\",obj2, \"oblTime2\",oblTime2, \"oblPlace2\",oblPlace2, \"oblManner2\",oblManner2, \"oblNull12\",oblNull12, \"verb2,\",verb2,\"####\")\n",
        "question_list=['چه‌ جور','چیست','مکانی','چه سان','چگونه','چطور','چه\\u200cطور','چه\\u200cسان','چه اندازه','چه\\u200cاندازه','چند','چه مقدار','چه\\u200cقدر','چه\\u200cمقدار','چندمی','چندم','چندمین','کجا','کدامی','چقدر','کدام','چه زمانی','چه وقت','چه','چه\\u200cکسی' , 'چه\\u200cفردی', 'چه\\u200cشخصی', 'چه کسی' ,'چه\\u200cکسانی' , 'چه\\u200cافرادی', 'چه\\u200cاشخاصی', 'چه کسانی','چه\\u200cچیزی' , 'چه\\u200cوسیله\\u200cای', 'چه\\u200cشی\\u200cای', 'چه چیزی' , 'چه وسیله ای', 'چه شی ای']\n",
        "type_Q=''\n",
        "Question_type=''\n",
        "res = [ele for ele in question_list if(ele in nsubj2)]\n",
        "if bool(res):\n",
        "  Question_type='سئوال در خصوص فاعل می باشد' \n",
        "  type_Q='nsubj'\n",
        "elif bool([ele for ele in question_list if(ele in obj2)]):\n",
        "  Question_type='سئوال در خصوص مفعول می باشد' \n",
        "  type_Q='obj'\n",
        "elif bool([ele for ele in question_list if(ele in oblTime2)]):\n",
        "  Question_type='سئوال در خصوص قید زمان می باشد' \n",
        "  type_Q='oblTime'\n",
        "\n",
        "elif bool([ele for ele in question_list if(ele in oblPlace2)]):\n",
        "  Question_type='سئوال در خصوص قید مکان می باشد' \n",
        "  type_Q='oblPlace'\n",
        "\n",
        "elif bool([ele for ele in question_list if(ele in oblManner2)]):\n",
        "  Question_type='سئوال در خصوص قید حالت می باشد' \n",
        "  type_Q='oblManner'\n",
        "\n",
        "elif bool([ele for ele in question_list if(ele in oblNull12)]):\n",
        "  Question_type='سئوال در خصوص قید ... می باشد' \n",
        "  print(res)\n",
        "  type_Q='oblNull1'\n",
        "\n",
        "elif bool([ele for ele in question_list if(ele in verb2)]):\n",
        "  Question_type='سئوال در خصوص فعل می باشد' \n",
        "  type_Q='verb'\n",
        "\n",
        "\n",
        "\n",
        "print(Question_type)\n",
        "print(type_Q)\n",
        "\n",
        "\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Thesis/QandA.db')\n",
        "\n",
        "sen_embedding = model1.encode(Text_Question)\n",
        "#sen_embedding.shape\n",
        "\n",
        "list_id=[]\n",
        "\n",
        "# code adapted from https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\n",
        "\n",
        "#Text_Question = 'سئوال را بپورسید ؟' #@param {type: 'string'}\n",
        "\n",
        "queries = [Text_Question]\n",
        "query_embeddings = model1.encode(queries)\n",
        "\n",
        "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
        "number_top_matches = 5 #@param {type: \"number\"}\n",
        "\n",
        "print(\"Semantic Search Results\")\n",
        "\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for idx, distance in results[0:number_top_matches]:\n",
        "        #print(idx,sentences[idx].strip(), \"(Cosine Score: %.5f)\" % (1-distance))\n",
        "        list_id.append(idx+1)\n",
        "        print(idx)\n",
        "\n",
        "\n",
        "print('list_id:::::::::::::::',list_id)\n",
        "Result_list=[]\n",
        "Result_Answer=[]\n",
        "Result_Sent=[]\n",
        "Result_sum=0\n",
        "for i in list_id:\n",
        "  print('iiiiiiiiiiiiiiii:::',i)\n",
        "  cursor = conn.execute(\"SELECT id, sentence, nsubj, obj, oblTime, oblPlace, oblManner, oblNull, verb FROM knowledge_data_full where id=?\",(i,))\n",
        "  \n",
        "\n",
        "  for row in cursor:\n",
        "    #print('################',row)\n",
        "    #print('id',row[0])\n",
        "    print('sentence',row[1])\n",
        "    #print('nsubj',row[2])\n",
        "    #print('obj',row[3])\n",
        "    #print('oblTime',row[4])\n",
        "    #print('oblPlace',row[5])\n",
        "    #print('oblManner',row[6])\n",
        "    #print('oblNull',row[7])\n",
        "    #print('verb',row[8])\n",
        "\n",
        "    if list_Question[2]!='' and row[2]:\n",
        "      print(list_Question[2], row[2])\n",
        "      nsubj_com=embedding_Comparison(list_Question[2],row[2])\n",
        "    else: nsubj_com=0 \n",
        "    if list_Question[3]!='' and row[3]:\n",
        "      print(list_Question[3], row[3])\n",
        "      obj_com=embedding_Comparison(list_Question[3],row[3])\n",
        "    else: obj_com=0 \n",
        "    if list_Question[4]!='' and row[4]:\n",
        "      print(list_Question[4], row[4])\n",
        "      oblTime_com=embedding_Comparison(list_Question[4],row[4])\n",
        "    else: oblTime_com=0 \n",
        "    if list_Question[5]!='' and row[5]:\n",
        "      print(list_Question[5], row[5])\n",
        "      oblPlace_com=embedding_Comparison(list_Question[5],row[5])\n",
        "    else: oblPlace_com=0 \n",
        "    if list_Question[6]!='' and row[6]:\n",
        "      print(list_Question[6], row[6])\n",
        "      #print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$4444444')\n",
        "      oblManner_com=embedding_Comparison(list_Question[6],row[6])\n",
        "    else: oblManner_com=0 \n",
        "    if list_Question[7]!='' and row[7]:\n",
        "      print(list_Question[7], row[7])\n",
        "      oblNull_com=embedding_Comparison(list_Question[7],row[7])\n",
        "    else: oblNull_com=0 \n",
        "    if list_Question[8]!='' and row[8]:\n",
        "      print(list_Question[8], row[8])\n",
        "      verb_com=embedding_Comparison(list_Question[8],row[8])\n",
        "    else: verb_com=0 \n",
        "\n",
        "    Result_sum,Result_Ans=Comp(row,type_Q,list_Question,nsubj_com,obj_com,oblTime_com,oblPlace_com,oblManner_com,oblNull_com,verb_com)\n",
        "    print(\"Result_sum : \", Result_sum)\n",
        "    print(\"Result_Ans : \", Result_Ans)\n",
        "    Result_Sent\n",
        "  Result_list.append(Result_sum)\n",
        "  Result_Answer.append(Result_Ans)\n",
        "  Result_Sent.append(row[1])\n",
        "\n",
        "conn.close()\n",
        "\n",
        "print(Question_type)\n",
        "print(type_Q)\n",
        "print(Result_sum)\n",
        "\n",
        "Result_list1=[]\n",
        "for res in Result_list:\n",
        "  Result_list1.append(res)\n",
        "Result_list1.sort()\n",
        "#print(\"Largest element 1 is:\", Result_list1[-1],\">> Result Answer 1:\",Result_Answer[Result_list.index(Result_list1[-1])],\" >> sentence Answer 1:\",Result_Sent[Result_list.index(Result_list1[-1])])\n",
        "#print(\"Largest element 2 is:\", Result_list1[-2],\">> Result Answer 2:\",Result_Answer[Result_list.index(Result_list1[-2])],\" >> sentence Answer 2:\",Result_Sent[Result_list.index(Result_list1[-2])])\n",
        "#print(\"Largest element 3 is:\", Result_list1[-3],\">> Result Answer 3:\",Result_Answer[Result_list.index(Result_list1[-3])],\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-3])])\n",
        "#print(\"Largest element is:\", max(Result_list))\n",
        "\n",
        "print(\"پاسخ سئوال شما : \" , sort1(Result_Answer[Result_list.index(Result_list1[-1])],Result_Sent[Result_list.index(Result_list1[-1])]))\n",
        "\n",
        "\n",
        "print(\"Largest element 1 is:\", Result_list1[-1],\">> Result Answer 1:\",Result_Answer[Result_list.index(Result_list1[-1])]\n",
        ",\" >> sentence Answer 1:\",Result_Sent[Result_list.index(Result_list1[-1])])\n",
        "print(\"Largest element 2 is:\", Result_list1[-2],\">> Result Answer 2:\",Result_Answer[Result_list.index(Result_list1[-2])]\n",
        ",\" >> sentence Answer 2:\",Result_Sent[Result_list.index(Result_list1[-2])])\n",
        "print(\"Largest element 3 is:\", Result_list1[-3],\">> Result Answer 3:\",Result_Answer[Result_list.index(Result_list1[-3])]\n",
        ",\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-3])])\n",
        "print(\"Largest element 4 is:\", Result_list1[-4],\">> Result Answer 1:\",Result_Answer[Result_list.index(Result_list1[-4])]\n",
        ",\" >> sentence Answer 4:\",Result_Sent[Result_list.index(Result_list1[-4])])\n",
        "print(\"Largest element 5 is:\", Result_list1[-5],\">> Result Answer 2:\",Result_Answer[Result_list.index(Result_list1[-5])]\n",
        ",\" >> sentence Answer 5:\",Result_Sent[Result_list.index(Result_list1[-5])])\n",
        "print(\"Largest element 6 is:\", Result_list1[-6],\">> Result Answer 3:\",Result_Answer[Result_list.index(Result_list1[-6])]\n",
        ",\" >> sentence Answer 6:\",Result_Sent[Result_list.index(Result_list1[-6])])\n",
        "print(\"***************@@@@@@@@****************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88D_5sSW-daY"
      },
      "outputs": [],
      "source": [
        "Result_list1=[]\n",
        "#print(Result_list)\n",
        "for res in Result_list:\n",
        "  Result_list1.append(res)\n",
        "Result_list1.sort()\n",
        "#print(\"Largest element 1 is:\", Result_list1[-1],\">> Result Answer 1:\",Result_Answer[Result_list.index(Result_list1[-1])],\" >> sentence Answer 1:\",Result_Sent[Result_list.index(Result_list1[-1])])\n",
        "#print(\"Largest element 2 is:\", Result_list1[-2],\">> Result Answer 2:\",Result_Answer[Result_list.index(Result_list1[-2])],\" >> sentence Answer 2:\",Result_Sent[Result_list.index(Result_list1[-2])])\n",
        "#print(\"Largest element 3 is:\", Result_list1[-3],\">> Result Answer 3:\",Result_Answer[Result_list.index(Result_list1[-3])],\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-3])])\n",
        "#print(\"Largest element is:\", max(Result_list))\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Largest element 1 is:\", Result_list1[-1],\">> Result Answer 1:\",wordcloud_def(Result_Answer[Result_list.index(Result_list1[-1])],stop_words)\n",
        ",\" >> sentence Answer 1:\",Result_Sent[Result_list.index(Result_list1[-1])])\n",
        "#print(\"Largest element 2 is:\", Result_list1[-2],\">> Result Answer 2:\",wordcloud_def(Result_Answer[Result_list.index(Result_list1[-2])],stop_words)\n",
        ",\" >> sentence Answer 2:\",Result_Sent[Result_list.index(Result_list1[-2])])\n",
        "#print(\"Largest element 3 is:\", Result_list1[-3],\">> Result Answer 3:\",wordcloud_def(Result_Answer[Result_list.index(Result_list1[-3])],stop_words)\n",
        ",\" >> sentence Answer 3:\",Result_Sent[Result_list.index(Result_list1[-3])])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moqYtS872996"
      },
      "outputs": [],
      "source": [
        "#print(Result_list)\n",
        "#print(\"Largest element is:\", max(Result_list))\n",
        "#print(Result_list[7])\n",
        "#print(Result_Answer)\n",
        "#print(Result_Sent[7])\n",
        "##print(sort1(Result_Answer[8],Result_Sent[8]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ze5DcdTXpaG6",
        "YjjaqlIbqdV2"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}